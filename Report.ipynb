{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f43fba6",
   "metadata": {},
   "source": [
    "## Report: Continuous Control project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5fa20",
   "metadata": {},
   "source": [
    "### Learning algorithm\n",
    "\n",
    "This project was solved using deep deterministic policy gradient (DDPG). DDPG is an actor-critic method for reinforcement learning, which applies well to continuous action spaces. Thus, the agent consists of four neural networks, two of which constitute the actor, and two of which constitute the critic. \n",
    "\n",
    "#### The actor\n",
    "\n",
    "The actor networks consist of 3 fully connected layers. The first layer receives the state as input, and the final layer outputs the actions. In the implementation, the hidden layers have 256 and 128 units. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "21d3de33",
   "metadata": {},
   "source": [
    "super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af12c5",
   "metadata": {},
   "source": [
    "#### The critic \n",
    "\n",
    "Like the actor, the critic networks have 3 fully connected layers. The critic networks input the state size and output the Q-value. In the implementation, fc1_units = 256 and fc2_units = 128."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9de7d36",
   "metadata": {},
   "source": [
    "super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d0e35",
   "metadata": {},
   "source": [
    "#### Agent instantiation \n",
    "\n",
    "The agent has local actor, target actor, local critic, and target critic networks. Initially, for each critic and actor, the local and target networks are identically initialised. \n",
    "\n",
    "In addition, a noise process is established which will set the agent to continually explore the action space (this is re-set at the beginning of each episode).\n",
    "\n",
    "Finally, the agent is allocated a \"memory\" - which is a store of previous {state, action, next_state, reward, done} tuples from which it will learn."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dee41397",
   "metadata": {},
   "source": [
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e61e8",
   "metadata": {},
   "source": [
    "#### Act, step, learn\n",
    "\n",
    "At the beginning of a new episode, the environment is reset and the agent observes the state. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "42b3a7ae",
   "metadata": {},
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations        # current state (for each agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20530eb9",
   "metadata": {},
   "source": [
    "Next, the environment uses the state to 'decide' which action to take. After acting, it observes the new environment and takes a step (see later)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8339deab",
   "metadata": {},
   "source": [
    "actions = agent.act(states)\n",
    "            \n",
    "env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "rewards = env_info.rewards                         # get reward (for each agent)\n",
    "dones = env_info.local_done                        # see if episode finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa0030",
   "metadata": {},
   "source": [
    "The agent acts by feeding the states to the local actor network, then adding noise:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "136cac39",
   "metadata": {},
   "source": [
    "def act(self, states, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "                \n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            noise_sample = self.noise.sample()\n",
    "            actions += noise_sample\n",
    "        return np.clip(actions, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f980a4d",
   "metadata": {},
   "source": [
    "Next, the agent 'steps' - stores the experience tuple and performs a learning update, which involves updating the local networks and soft updating the target networks according to hyperparameter TAU. The effect is that the target networks change more slowly."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf693bba",
   "metadata": {},
   "source": [
    "def step(self, timestep, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            self.memory.add(states[i,:], actions[i,:], rewards[i], next_states[i,:], dones[i])\n",
    "        \n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "            \n",
    "def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)  \n",
    "\n",
    "def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b8d47",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "___BUFFER_SIZE = int(1e4)___ The buffer size determines how many experience tuples are stored in the agent's memory and from which it can sample to learn.\n",
    "\n",
    "___BATCH_SIZE = 128___ Batch size determines how many experience tuples are sampled from the memory buffer during a learning update.\n",
    "\n",
    "___GAMMA = 0.99___ Gamma is the 'discount factor' - meaning it determines the weight given to existing network parameters compared to the newly observed rewards.\n",
    "\n",
    "___TAU = 1e-3___ Tau determines the weight given to the local network parameters when updating the target networks. \n",
    "\n",
    "___LR_ACTOR = 1e-4, LR_CRITIC = 4e-4___ The learning rates of the actor and critic. I changed these around quite a bit and found that the model performance was very sensitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42ccf68",
   "metadata": {},
   "source": [
    "### Performance \n",
    "\n",
    "This learning algorithm solved the environment with 20 agents in 122 agents (mean score across all 20 agents over the last 100 trials of >= 30)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e0307",
   "metadata": {},
   "source": [
    "![](Rewards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6842d",
   "metadata": {},
   "source": [
    "### Future directions\n",
    "\n",
    "___Single agent:___ Try to solve the task with a single agent. In theory this should be implementable with the same learning algorithm and model architectures used here, but may take longer.\n",
    "\n",
    "___Prioritized replay:___ Try storing memories in a probability-dependent manner to prioritize learning from rare state-action tuples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
